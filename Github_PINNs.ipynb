{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bshi04/deep-learning-fa-2025/blob/main/Github_PINNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCMDUQGYXQeL"
      },
      "source": [
        "In this project, we solve the steady-state 2D convection-diffusion equation on a unit square domain, discretized using a $400 \\times 400$ grid. The solution is governed by the following equation,\n",
        "$$\n",
        "-\\varepsilon \\nabla^2 u(x,y) + \\mathbf{U}\\cdot\\nabla u(x,y) = f(x,y),\n",
        "$$\n",
        "where $u(x,y)$ is the scalar field of interest, $\\varepsilon$ denotes thed iffusion coefficient, $\\mathbf{U}$ is the constant advection velocity vector, and $f(x,y)$ is a spatially varying source term.\n",
        "\n",
        "Reference solutions are generated using a custom finite-difference solver implemented in Python. The diffusion operator is approximated using a second-order central-difference scheme to ensure high spatial accuracy. A Dirichlet boundary condition,\n",
        "$$\n",
        "u = 0,\n",
        "$$\n",
        "is imposed on the left and right walls of the domain. A Neumann (zero-flux) boundary condition,\n",
        "$$\n",
        "\\frac{\\partial u}{\\partial n} = 0,\n",
        "$$\n",
        "is applied on all remaining boundaries. For the PINN experiments, collocation points are sampled throughout the same square domain. The governing PDE is enforced at each collocation point, and the resulting predictions are compared against the numerically computed fine-mesh solution to evaluate model accuracy and convergence behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4d1ee8d"
      },
      "source": [
        "### 1a. Generate Reference Data\n",
        "\n",
        "Defines relevant constants and paramters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqyAy8grzCw6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import lil_matrix, csc_matrix\n",
        "from scipy.sparse.linalg import spsolve\n",
        "\n",
        "# Initialize coefficients\n",
        "U = 0.1\n",
        "eps = 0.01\n",
        "\n",
        "#  grid\n",
        "Nx, Ny = 400, 400\n",
        "Lx, Ly = 1.0, 1.0\n",
        "\n",
        "# variable source\n",
        "s0 = 0.1\n",
        "beta = 1.0\n",
        "\n",
        "# f(x,y) decreasing in y\n",
        "def source_fn(y):\n",
        "    if isinstance(y, torch.Tensor):\n",
        "        return s0 * torch.exp(-beta * y) # supports GPU\n",
        "    return s0 * np.exp(-beta * y) # supports CPU\n",
        "\n",
        "# PINN params\n",
        "num_epochs = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FSFlXWGa2tn"
      },
      "source": [
        "### 1b. Generate Reference Data using Finite Difference Method\n",
        "\n",
        "Finite-difference solver to generate the reference solution, `U_ref`, and the grid coordinates `X`, `Y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWzn-OFYzUy6"
      },
      "outputs": [],
      "source": [
        "# Finite Difference\n",
        "def fd_solver(U: float, eps: float, Nx: int, Ny: int, Lx: float, Ly: float):\n",
        "    \"\"\"Compute finite difference solution on specified grid.\n",
        "\n",
        "    Args:\n",
        "        U (float): Convection coefficient.\n",
        "        eps (float): Difffusion coefficient.\n",
        "        Nx (int): number of grid points in x-direction.\n",
        "        Ny (int): number of grid points in y-direction.\n",
        "        Lx (float): length of domain in x-direction.\n",
        "        Ly (float): length of domain in y-direction.\n",
        "\n",
        "    Returns:\n",
        "        dict: X-coords, Y-coords, and solution u on the grid.\n",
        "    \"\"\"\n",
        "    hx, hy = Lx / Nx, Ly / Ny\n",
        "    x = np.linspace(0.0, Lx, Nx + 1)\n",
        "    y = np.linspace(0.0, Ly, Ny + 1)\n",
        "\n",
        "    # map (i,j) to global index\n",
        "    def idx(i, j):\n",
        "        return j * (Nx + 1) + i\n",
        "\n",
        "    Ntot = (Nx + 1) * (Ny + 1)\n",
        "    A = lil_matrix((Ntot, Ntot))\n",
        "    f_hat = np.zeros(Ntot)\n",
        "\n",
        "    # interior points\n",
        "    for j in range(1, Ny):\n",
        "        for i in range(1, Nx):\n",
        "            p = idx(i, j)\n",
        "\n",
        "            # convection term: U * u_x (central difference)\n",
        "            A[p, idx(i + 1, j)] += U / (2.0 * hx)\n",
        "            A[p, idx(i - 1, j)] -= U / (2.0 * hx)\n",
        "\n",
        "            # diffusion term: -eps * (u_xx + u_yy)\n",
        "            A[p, idx(i + 1, j)] += -eps / hx**2\n",
        "            A[p, idx(i - 1, j)] += -eps / hx**2\n",
        "            A[p, p] += 2.0 * eps / hx**2\n",
        "\n",
        "            # u_yy part\n",
        "            A[p, idx(i, j + 1)] += -eps / hy**2\n",
        "            A[p, idx(i, j - 1)] += -eps / hy**2\n",
        "            A[p, p] += 2.0 * eps / hy**2\n",
        "\n",
        "            # source term f(y_j)\n",
        "            f_hat[p] = source_fn(y[j])\n",
        "\n",
        "    # left wall: Dirichlet u=0\n",
        "    for j in range(Ny + 1):\n",
        "        p = idx(0, j)\n",
        "        A[p, :] = 0.0\n",
        "        A[p, p] = 1.0\n",
        "        f_hat[p] = 0.0\n",
        "\n",
        "    # right wall: Dirichlet u=0\n",
        "    for j in range(Ny + 1):\n",
        "        p = idx(Nx, j)\n",
        "        A[p, :] = 0.0\n",
        "        A[p, p] = 1.0\n",
        "        f_hat[p] = 0.0\n",
        "\n",
        "    # bottom: Neumann du/dy=0\n",
        "    for i in range(1, Nx):\n",
        "        p = idx(i, 0)\n",
        "        A[p, :] = 0.0\n",
        "        A[p, idx(i, 0)] = -1.0 / hy\n",
        "        A[p, idx(i, 1)] =  1.0 / hy\n",
        "        f_hat[p] = 0.0\n",
        "\n",
        "    # top: Neumann du/dy=0\n",
        "    for i in range(1, Nx):\n",
        "        p = idx(i, Ny)\n",
        "        A[p, :] = 0.0\n",
        "        A[p, idx(i, Ny - 1)] = -1.0 / hy\n",
        "        A[p, idx(i, Ny)]     =  1.0 / hy\n",
        "        f_hat[p] = 0.0\n",
        "\n",
        "    u = spsolve(csc_matrix(A), f_hat).reshape((Ny + 1, Nx + 1))\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "\n",
        "    return {\"X\": X, \"Y\": Y, \"u\": u}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c. PINN Architectures"
      ],
      "metadata": {
        "id": "v_undRRC1NfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Generic MLP used as a building block for advection and diffusion subnetworks.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim=2,\n",
        "        out_dim=1,\n",
        "        hidden_dim=64,\n",
        "        num_hidden_layers=6,\n",
        "        act_fn=nn.Tanh(), # additional experiments modify silu\n",
        "    ):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        layers.append(act_fn)\n",
        "        for _hidden in range(num_hidden_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(act_fn)\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "# vanilla pinn\n",
        "class PINN(nn.Module):\n",
        "    \"\"\"Standard fully connected PINN network to approximate u(x, y).\"\"\"\n",
        "    def __init__(self,\n",
        "                 in_dim=2,\n",
        "                 out_dim=1,\n",
        "                 hidden_dim=64,\n",
        "                 num_hidden_layers=6):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(in_dim, hidden_dim))\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        for _hidden in range(num_hidden_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(hidden_dim, out_dim))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x) # x: [N, 2] = (x, y)\n",
        "\n",
        "# gated approach\n",
        "class SplitPINN(nn.Module):\n",
        "    \"\"\"Two-branch PINN to seperately capture advection and diffusion terms.\n",
        "       Contributions are re-coupled through a parallel gating branch.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim=2,\n",
        "        out_dim=1,\n",
        "        hidden_dim=45,\n",
        "        num_hidden_layers=6,\n",
        "        silu_bool=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # defaults to silu split on adv branch due to improved convergence\n",
        "        if silu_bool:\n",
        "            self.adv_net  = MLP(in_dim, out_dim, hidden_dim, num_hidden_layers, act_fn=nn.SiLU())\n",
        "        else:\n",
        "            self.adv_net  = MLP(in_dim, out_dim, hidden_dim, num_hidden_layers)\n",
        "\n",
        "        self.diff_net = MLP(in_dim, out_dim, hidden_dim, num_hidden_layers)\n",
        "\n",
        "        # small subnet to learn fusion gate\n",
        "        self.gate_net = nn.Sequential(\n",
        "            nn.Linear(2, 20),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(20, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        u_adv  = self.adv_net(x)\n",
        "        u_diff = self.diff_net(x)\n",
        "        gate = self.gate_net(x)\n",
        "\n",
        "        u = (1 - gate) * u_diff + gate * u_adv\n",
        "        return u\n",
        "\n",
        "# addition re-coupling approach\n",
        "class AddSplitPINN(nn.Module):\n",
        "    \"\"\"Two-branch PINN to seperately capture advection and diffusion terms.\n",
        "       Contributions are re-coupled via addition.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim=2,\n",
        "        out_dim= 1,\n",
        "        hidden_dim=45,\n",
        "        num_hidden_layers=6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.adv_net  = MLP(in_dim, out_dim, hidden_dim, num_hidden_layers)\n",
        "        self.diff_net = MLP(in_dim, out_dim, hidden_dim, num_hidden_layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        u_adv  = self.adv_net(x)\n",
        "        u_diff = self.diff_net(x)\n",
        "        u = u_adv + u_diff\n",
        "        return u\n",
        "\n",
        "\n",
        "# feature mixing approach\n",
        "class MixedPINN(nn.Module):\n",
        "  \"\"\"Advection and diffusion branches each produce feature vectors that are concatenated\n",
        "     and mapped to the scalar output u through a final linear layer.\n",
        "  \"\"\"\n",
        "  def __init__(\n",
        "        self,\n",
        "        in_dim=2,\n",
        "        out_dim=1,\n",
        "        hidden_dim=44,\n",
        "        num_hidden_layers=6,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.adv_features  = MLP(\n",
        "            in_dim=in_dim,\n",
        "            out_dim=10,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_hidden_layers=num_hidden_layers,\n",
        "            act_fn=nn.SiLU()\n",
        "        )\n",
        "\n",
        "        self.diff_features = MLP(\n",
        "            in_dim=in_dim,\n",
        "            out_dim=10,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_hidden_layers=num_hidden_layers,\n",
        "        )\n",
        "\n",
        "        self.final_layer = nn.Linear(20, 1)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        f_adv  = self.adv_features(x)\n",
        "        f_diff = self.diff_features(x)\n",
        "        combined = torch.cat([f_adv, f_diff], dim=1) # feature vectors concatenated\n",
        "        return self.final_layer(combined)\n"
      ],
      "metadata": {
        "id": "GFGFD-vT1aiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9ujeiYJQik0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}